{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from squeezer.criterion import distill_loss\n",
    "from squeezer.distiller import Distiller\n",
    "from squeezer.policy import AbstractDistillationPolicy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa012f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0xDEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, n_epochs: int = 200):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        for i, (data, labels) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac141139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for data, labels in loader:\n",
    "        outputs = model(data).argmax(-1)\n",
    "        preds.append(outputs)\n",
    "        targets.append(labels)\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    print(classification_report(targets, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e873c58",
   "metadata": {},
   "source": [
    "# Models\n",
    "Объявляем модель-учитель побольше и модель-ученик поменьше.  \n",
    "Тип возвращаемого значения должен наследоваться от класса `ModelOutput` (или быть им)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.network(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.network(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883454a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e871bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(num_features: int = 64, num_classes: int = 4,\n",
    "                batch_size: int = 64, train_size: float = 0.75):\n",
    "    x, y = make_classification(\n",
    "        1000, num_features,\n",
    "        n_classes=num_classes,\n",
    "        n_informative=int(num_features * 0.9),\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=4.0,\n",
    "        random_state=0xDEAD\n",
    "    )\n",
    "    dataset = TensorDataset(\n",
    "        torch.from_numpy(x).float(),\n",
    "        torch.from_numpy(y).long()\n",
    "    )\n",
    "    dataset_length = len(x)\n",
    "    train_size = int(dataset_length * train_size)\n",
    "    val_size = dataset_length - train_size\n",
    "    train, val = random_split(dataset, [train_size, val_size])\n",
    "    return DataLoader(train, batch_size=batch_size), DataLoader(val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 128\n",
    "num_classes = 5\n",
    "\n",
    "train_loader, val_loader = get_loaders(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba411b56",
   "metadata": {},
   "source": [
    "# Train Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = Teacher(num_features, num_classes, hidden_size=128)\n",
    "\n",
    "train(teacher, train_loader, n_epochs=n_epochs)\n",
    "evaluate(teacher, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c45723",
   "metadata": {},
   "source": [
    "# Train Student model without distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51484f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "train(student, train_loader, n_epochs=n_epochs)\n",
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157f1cb",
   "metadata": {},
   "source": [
    "# Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c275478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDistiller(Distiller):\n",
    "    def teacher_forward(self, batch):\n",
    "        return self.teacher(batch[0])\n",
    "    \n",
    "    def student_forward(self, batch):\n",
    "        return self.student(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bcd32",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6ef24",
   "metadata": {},
   "source": [
    "## Basic distillation policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class BasicDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def __init__(self, temperature: float = 1.0, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, teacher_output, student_output, batch, epoch: int) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        loss_kld, loss_ce, overall = distill_loss(\n",
    "            teacher_logits=teacher_output,\n",
    "            student_logits=student_output,\n",
    "            labels=batch[1],\n",
    "            temperature=self.temperature,\n",
    "            alpha=self.alpha\n",
    "        )\n",
    "        loss_dict = {\n",
    "            'kld': loss_kld.item(),\n",
    "            'cross_entropy': loss_ce.item(),\n",
    "            'overall': overall.item(),\n",
    "        }\n",
    "        return overall, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2390a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "# Инициализируем политику функции потерь для дистилляции.\n",
    "# В этом примере используется стандартная политика, при которой\n",
    "# ученик учится сразу на распределение логитов учителя (KLD) и на мейнстрим задачу (CE).\n",
    "# Для кастомизации политики, например, для использования других функций потерь или\n",
    "# добавления адаптеров между аутпутами моделей, необходимо наследоваться от класса `AbstractDistillationPolicy`\n",
    "policy = BasicDistillationPolicy(temperature=1.2, alpha=0.5)\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, val_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbfae4",
   "metadata": {},
   "source": [
    "## Custom policy (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class SingleMSEDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def forward(self, teacher_output, student_output, batch, epoch) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        loss_mse = nn.functional.mse_loss(student_output, teacher_output)\n",
    "        loss_dict = {'mse': loss_mse.item()}\n",
    "        return loss_mse, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32384e9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "policy = SingleMSEDistillationPolicy()\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, val_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc182f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc49bb",
   "metadata": {},
   "source": [
    "## Advanced policy\n",
    "**CE + MSE with scale decay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fabb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class AdvancedDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def __init__(self, n_epochs: int, adapter_mapping: Optional[Tuple[int, int]] = None):\n",
    "        super().__init__()\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def forward(self, teacher_output, student_output, batch, epoch) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        alpha = math.exp((epoch + 1) / self.n_epochs)\n",
    "        loss_mse = nn.functional.mse_loss(student_output, teacher_output)\n",
    "        loss_ce = nn.functional.cross_entropy(student_output, batch[1])\n",
    "        overall = loss_mse * alpha + loss_ce * (1 - alpha)\n",
    "        scalars_dict = {\n",
    "            'mse': loss_mse.item(),\n",
    "            'cross_entropy': loss_ce.item(),\n",
    "            'overall': overall.item(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "        return overall, scalars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cae039",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "policy = AdvancedDistillationPolicy(n_epochs)\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, val_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(student, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
