{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8a5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from squeezer.criterion import distill_loss\n",
    "from squeezer.distiller import Distiller\n",
    "from squeezer.policy import AbstractDistillationPolicy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853516ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa930876350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0xDEAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea568202",
   "metadata": {},
   "source": [
    "# Models\n",
    "Объявляем модель-учитель побольше и модель-ученик поменьше.  \n",
    "Тип возвращаемого значения должен наследоваться от класса `ModelOutput` (или быть им)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a54c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.network(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32575b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.network(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165f112",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(length: int = 10000, num_features: int = 20, num_classes: int = 4, batch_size: int = 64):\n",
    "    data_tensor = torch.randn(length, num_features)\n",
    "    target_tensor = torch.randint(high=num_classes, size=(length,))\n",
    "    dataset = TensorDataset(data_tensor, target_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291e4df",
   "metadata": {},
   "source": [
    "# Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328c6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDistiller(Distiller):\n",
    "    def teacher_forward(self, batch):\n",
    "        return self.teacher(batch[0])\n",
    "    \n",
    "    def student_forward(self, batch):\n",
    "        return self.student(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55868cfa",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe460e",
   "metadata": {},
   "source": [
    "## Basic distillation policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3468d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class BasicDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def __init__(self, temperature: float = 1.0, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, teacher_output, student_output, batch, epoch: int) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        loss_kld, loss_ce, overall = distill_loss(\n",
    "            teacher_logits=teacher_output,\n",
    "            student_logits=student_output,\n",
    "            labels=batch[1],\n",
    "            temperature=self.temperature,\n",
    "            alpha=self.alpha\n",
    "        )\n",
    "        loss_dict = {\n",
    "            'kld': loss_kld.item(),\n",
    "            'cross_entropy': loss_ce.item(),\n",
    "            'overall': overall.item(),\n",
    "        }\n",
    "        return overall, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219c044f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: kld=0.00027   cross_entropy=0.00948   overall=0.00027        \n",
      "Epoch 1: kld=0.00013   cross_entropy=0.00920   overall=0.00013        \n",
      "Epoch 2: kld=0.00004   cross_entropy=0.00908   overall=0.00004      \n",
      "Epoch 3: kld=0.00001   cross_entropy=0.00906   overall=0.00001        \n",
      "Epoch 4: kld=0.00000   cross_entropy=0.00907   overall=0.00000        \n",
      "Epoch 5: kld=0.00000   cross_entropy=0.00908   overall=0.00000        \n",
      "Epoch 6: kld=0.00000   cross_entropy=0.00910   overall=0.00000        \n",
      "Epoch 7: kld=0.00000   cross_entropy=0.00911   overall=0.00000        \n",
      "Epoch 8: kld=0.00000   cross_entropy=0.00912   overall=0.00000        \n",
      "Epoch 9: kld=0.00000   cross_entropy=0.00913   overall=0.00000        \n",
      "Epoch 10: kld=0.00000   cross_entropy=0.00914   overall=0.00000        \n",
      "Epoch 11: kld=0.00000   cross_entropy=0.00914   overall=0.00000        \n",
      "Epoch 12: kld=0.00000   cross_entropy=0.00915   overall=0.00000        \n",
      "Epoch 13: kld=0.00000   cross_entropy=0.00915   overall=0.00000        \n",
      "Epoch 14: kld=0.00000   cross_entropy=0.00916   overall=0.00000        \n",
      "Epoch 15: kld=0.00000   cross_entropy=0.00916   overall=0.00000      \n",
      "Epoch 16: kld=0.00000   cross_entropy=0.00916   overall=0.00000        \n",
      "Epoch 17: kld=0.00000   cross_entropy=0.00916   overall=0.00000        \n",
      "Epoch 18: kld=0.00000   cross_entropy=0.00916   overall=0.00000      \n",
      "Epoch 19: kld=0.00000   cross_entropy=0.00916   overall=0.00000      \n",
      "Epoch 20: kld=0.00000   cross_entropy=0.00917   overall=0.00000        \n",
      "Epoch 21: kld=0.00000   cross_entropy=0.00917   overall=0.00000        \n",
      "Epoch 22: kld=0.00000   cross_entropy=0.00917   overall=0.00000        \n",
      "Epoch 23: kld=0.00000   cross_entropy=0.00917   overall=0.00000        \n",
      "Epoch 24: kld=0.00000   cross_entropy=0.00917   overall=0.00000        \n",
      "Epoch 25: kld=0.00000   cross_entropy=0.00917   overall=0.00000        \n",
      "Epoch 26: kld=0.00000   cross_entropy=0.00918   overall=0.00000        \n",
      "Epoch 27: kld=0.00000   cross_entropy=0.00918   overall=0.00000        \n",
      "Epoch 28: kld=0.00000   cross_entropy=0.00918   overall=0.00000      \n",
      "Epoch 29: kld=0.00000   cross_entropy=0.00918   overall=0.00000        \n",
      "Epoch 30: kld=0.00000   cross_entropy=0.00918   overall=0.00000        \n",
      "Epoch 31: kld=0.00000   cross_entropy=0.00918   overall=0.00000        \n",
      "Epoch 32: kld=0.00000   cross_entropy=0.00918   overall=0.00000        \n",
      "Epoch 33: kld=0.00000   cross_entropy=0.00919   overall=0.00000        \n",
      "Epoch 34: kld=0.00000   cross_entropy=0.00919   overall=0.00000      \n",
      "Epoch 35: kld=0.00000   cross_entropy=0.00919   overall=0.00000        \n",
      "Epoch 36: kld=0.00000   cross_entropy=0.00919   overall=0.00000        \n",
      "Epoch 37: kld=0.00000   cross_entropy=0.00919   overall=0.00000      \n",
      "Epoch 38: kld=0.00000   cross_entropy=0.00919   overall=0.00000      \n",
      "Epoch 39: kld=0.00000   cross_entropy=0.00919   overall=0.00000        \n",
      "Epoch 40: kld=0.00000   cross_entropy=0.00919   overall=0.00000        \n",
      "Epoch 41: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 42: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 43: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 44: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 45: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 46: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 47: kld=0.00000   cross_entropy=0.00920   overall=0.00000        \n",
      "Epoch 48: kld=0.00000   cross_entropy=0.00920   overall=0.00000      \n",
      "Epoch 49: kld=0.00000   cross_entropy=0.00920   overall=0.00000      \n"
     ]
    }
   ],
   "source": [
    "input_size = 32\n",
    "num_classes = 4\n",
    "n_epochs = 50\n",
    "\n",
    "train_loader = get_loader(num_features=input_size, num_classes=num_classes)\n",
    "teacher = Teacher(input_size, num_classes, hidden_size=10)\n",
    "student = Student(input_size, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "# Инициализируем политику функции потерь для дистилляции.\n",
    "# В этом примере используется стандартная политика, при которой\n",
    "# ученик учится сразу на распределение логитов учителя (KLD) и на мейнстрим задачу (CE).\n",
    "# Для кастомизации политики, например, для использования других функций потерь или\n",
    "# добавления адаптеров между аутпутами моделей, необходимо наследоваться от класса `AbstractDistillationPolicy`\n",
    "policy = BasicDistillationPolicy(temperature=0.1, alpha=1.0)\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caebe14",
   "metadata": {},
   "source": [
    "### Student vs Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10db4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher output:\n",
      "tensor([[-0.1217,  0.1959, -0.2506, -0.5039],\n",
      "        [-0.1324,  0.0405, -0.1797, -0.4393],\n",
      "        [-0.0844,  0.0537, -0.1318, -0.4583],\n",
      "        [-0.1500,  0.1800, -0.2762, -0.5421],\n",
      "        [-0.1290,  0.1328, -0.2370, -0.5321]])\n",
      "Student output:\n",
      "tensor([[-0.1160,  0.1547, -0.2141, -0.4610],\n",
      "        [ 0.1311,  0.2426,  0.1211, -0.1439],\n",
      "        [ 0.1691,  0.3280,  0.0969, -0.2133],\n",
      "        [-0.0200,  0.3092, -0.1212, -0.3685],\n",
      "        [ 0.3329,  0.6296,  0.2104, -0.0705]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    random_input = torch.randn(5, input_size)\n",
    "    teacher_output = teacher(random_input)\n",
    "    student_output = student(random_input)\n",
    "    print('Teacher output:')\n",
    "    print(teacher_output)\n",
    "    print('Student output:')\n",
    "    print(student_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20491608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.07529\n"
     ]
    }
   ],
   "source": [
    "mse = nn.functional.mse_loss(student_output, teacher_output).item()\n",
    "print(f'MSE: {mse:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96344b2",
   "metadata": {},
   "source": [
    "## Custom policy (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b62b1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class SingleMSEDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def forward(self, teacher_output, student_output, batch, epoch) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        loss_mse = nn.functional.mse_loss(student_output, teacher_output)\n",
    "        loss_dict = {'mse': loss_mse.item()}\n",
    "        return loss_mse, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc70ee6e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: mse=0.00135                                                  \n",
      "Epoch 1: mse=0.00064                                                  \n",
      "Epoch 2: mse=0.00028                                                  \n",
      "Epoch 3: mse=0.00012                                                  \n",
      "Epoch 4: mse=0.00005                                                  \n",
      "Epoch 5: mse=0.00002                                                  \n",
      "Epoch 6: mse=0.00001                                                  \n",
      "Epoch 7: mse=0.00001                                                  \n",
      "Epoch 8: mse=0.00001                                                  \n",
      "Epoch 9: mse=0.00001                                                  \n",
      "Epoch 10: mse=0.00001                                                  \n",
      "Epoch 11: mse=0.00001                                                  \n",
      "Epoch 12: mse=0.00001                                                  \n",
      "Epoch 13: mse=0.00001                                                  \n",
      "Epoch 14: mse=0.00001                                                  \n",
      "Epoch 15: mse=0.00001                                                  \n",
      "Epoch 16: mse=0.00001                                                  \n",
      "Epoch 17: mse=0.00001                                                  \n",
      "Epoch 18: mse=0.00001                                                  \n",
      "Epoch 19: mse=0.00001                                                  \n",
      "Epoch 20: mse=0.00001                                                  \n",
      "Epoch 21: mse=0.00001                                                  \n",
      "Epoch 22: mse=0.00001                                                  \n",
      "Epoch 23: mse=0.00001                                                  \n",
      "Epoch 24: mse=0.00001                                                  \n",
      "Epoch 25: mse=0.00001                                                  \n",
      "Epoch 26: mse=0.00001                                                  \n",
      "Epoch 27: mse=0.00001                                                  \n",
      "Epoch 28: mse=0.00001                                                  \n",
      "Epoch 29: mse=0.00001                                                 \n",
      "Epoch 30: mse=0.00001                                                  \n",
      "Epoch 31: mse=0.00001                                                  \n",
      "Epoch 32: mse=0.00001                                                  \n",
      "Epoch 33: mse=0.00001                                                  \n",
      "Epoch 34: mse=0.00001                                                  \n",
      "Epoch 35: mse=0.00001                                                  \n",
      "Epoch 36: mse=0.00001                                                  \n",
      "Epoch 37: mse=0.00001                                                  \n",
      "Epoch 38: mse=0.00001                                                  \n",
      "Epoch 39: mse=0.00001                                                  \n",
      "Epoch 40: mse=0.00001                                                  \n",
      "Epoch 41: mse=0.00001                                                  \n",
      "Epoch 42: mse=0.00001                                                  \n",
      "Epoch 43: mse=0.00001                                                  \n",
      "Epoch 44: mse=0.00001                                                  \n",
      "Epoch 45: mse=0.00001                                                  \n",
      "Epoch 46: mse=0.00001                                                  \n",
      "Epoch 47: mse=0.00001                                                  \n",
      "Epoch 48: mse=0.00001                                                  \n",
      "Epoch 49: mse=0.00001                                                  \n"
     ]
    }
   ],
   "source": [
    "input_size = 32\n",
    "num_classes = 4\n",
    "n_epochs = 50\n",
    "\n",
    "train_loader = get_loader(num_features=input_size, num_classes=num_classes)\n",
    "teacher = Teacher(input_size, num_classes, hidden_size=10)\n",
    "student = Student(input_size, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "policy = SingleMSEDistillationPolicy()\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3184da",
   "metadata": {},
   "source": [
    "### Student vs Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99676293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher output:\n",
      "tensor([[ 0.2362,  0.2263,  0.1356, -0.0325],\n",
      "        [ 0.1388,  0.2534,  0.2083, -0.0543],\n",
      "        [ 0.2643,  0.1796,  0.0818,  0.0101],\n",
      "        [ 0.2930,  0.1302, -0.0004,  0.0206],\n",
      "        [ 0.2378,  0.1847,  0.1942, -0.0717]])\n",
      "Student output:\n",
      "tensor([[ 0.2168,  0.2256,  0.1496, -0.0585],\n",
      "        [ 0.2011,  0.2101,  0.1793, -0.0609],\n",
      "        [ 0.2844,  0.1557,  0.0257,  0.0510],\n",
      "        [ 0.3263,  0.1230,  0.0051,  0.0220],\n",
      "        [ 0.2074,  0.1624,  0.1819, -0.0587]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    random_input = torch.randn(5, input_size)\n",
    "    teacher_output = teacher(random_input)\n",
    "    student_output = student(random_input)\n",
    "    print('Teacher output:')\n",
    "    print(teacher_output)\n",
    "    print('Student output:')\n",
    "    print(student_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801ebe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00083\n"
     ]
    }
   ],
   "source": [
    "mse = nn.functional.mse_loss(student_output, teacher_output).item()\n",
    "print(f'MSE: {mse:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75f393",
   "metadata": {},
   "source": [
    "## Advanced policy\n",
    "**1. CE + MSE with scale decay**  \n",
    "**2. Layer adapter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15b99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class AdvancedDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def __init__(self, n_epochs: int, adapter_mapping: Optional[Tuple[int, int]] = None):\n",
    "        super().__init__()\n",
    "        self.n_epochs = n_epochs\n",
    "        self.adapter = nn.Identity() if adapter_mapping is None else nn.Linear(*adapter_mapping)\n",
    "    \n",
    "    def forward(self, teacher_output, student_output, batch, epoch) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        alpha = (epoch + 1) / self.n_epochs\n",
    "        projected_teacher_logits = self.adapter(teacher_output)\n",
    "\n",
    "        loss_mse = nn.functional.mse_loss(student_output, projected_teacher_logits)\n",
    "        loss_ce = nn.functional.cross_entropy(student_output, batch[1])\n",
    "        overall = loss_mse * alpha + loss_ce * (1 - alpha)\n",
    "        scalars_dict = {\n",
    "            'mse': loss_mse.item(),\n",
    "            'cross_entropy': loss_ce.item(),\n",
    "            'overall': overall.item(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "        return overall, scalars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fcf0b5e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: mse=0.00238   cross_entropy=0.00998   overall=0.00982   alpha=0.00013\n",
      "Epoch 1: mse=0.00196   cross_entropy=0.00961   overall=0.00930   alpha=0.00026\n",
      "Epoch 2: mse=0.00164   cross_entropy=0.00933   overall=0.00887   alpha=0.00038\n",
      "Epoch 3: mse=0.00139   cross_entropy=0.00913   overall=0.00851   alpha=0.00051\n",
      "Epoch 4: mse=0.00120   cross_entropy=0.00898   overall=0.00820   alpha=0.00064\n",
      "Epoch 5: mse=0.00105   cross_entropy=0.00887   overall=0.00793   alpha=0.00077\n",
      "Epoch 6: mse=0.00091   cross_entropy=0.00880   overall=0.00769   alpha=0.00090\n",
      "Epoch 7: mse=0.00079   cross_entropy=0.00875   overall=0.00748   alpha=0.00103\n",
      "Epoch 8: mse=0.00069   cross_entropy=0.00872   overall=0.00727   alpha=0.00115\n",
      "Epoch 9: mse=0.00059   cross_entropy=0.00870   overall=0.00708   alpha=0.00128\n",
      "Epoch 10: mse=0.00051   cross_entropy=0.00869   overall=0.00689   alpha=0.00141\n",
      "Epoch 11: mse=0.00043   cross_entropy=0.00869   overall=0.00670   alpha=0.00154\n",
      "Epoch 12: mse=0.00036   cross_entropy=0.00869   overall=0.00652   alpha=0.00167\n",
      "Epoch 13: mse=0.00031   cross_entropy=0.00870   overall=0.00635   alpha=0.00179\n",
      "Epoch 14: mse=0.00026   cross_entropy=0.00871   overall=0.00617   alpha=0.00192\n",
      "Epoch 15: mse=0.00022   cross_entropy=0.00872   overall=0.00600   alpha=0.00205\n",
      "Epoch 16: mse=0.00019   cross_entropy=0.00873   overall=0.00583   alpha=0.00218\n",
      "Epoch 17: mse=0.00016   cross_entropy=0.00874   overall=0.00565   alpha=0.00231\n",
      "Epoch 18: mse=0.00014   cross_entropy=0.00876   overall=0.00548   alpha=0.00244\n",
      "Epoch 19: mse=0.00012   cross_entropy=0.00877   overall=0.00531   alpha=0.00256\n",
      "Epoch 20: mse=0.00011   cross_entropy=0.00879   overall=0.00514   alpha=0.00269\n",
      "Epoch 21: mse=0.00009   cross_entropy=0.00880   overall=0.00497   alpha=0.00282\n",
      "Epoch 22: mse=0.00008   cross_entropy=0.00882   overall=0.00480   alpha=0.00295\n",
      "Epoch 23: mse=0.00008   cross_entropy=0.00883   overall=0.00463   alpha=0.00308\n",
      "Epoch 24: mse=0.00007   cross_entropy=0.00885   overall=0.00446   alpha=0.00321\n",
      "Epoch 25: mse=0.00006   cross_entropy=0.00886   overall=0.00428   alpha=0.00333\n",
      "Epoch 26: mse=0.00005   cross_entropy=0.00887   overall=0.00411   alpha=0.00346\n",
      "Epoch 27: mse=0.00005   cross_entropy=0.00889   overall=0.00394   alpha=0.00359\n",
      "Epoch 28: mse=0.00004   cross_entropy=0.00890   overall=0.00376   alpha=0.00372\n",
      "Epoch 29: mse=0.00004   cross_entropy=0.00891   overall=0.00359   alpha=0.00385\n",
      "Epoch 30: mse=0.00003   cross_entropy=0.00893   overall=0.00341   alpha=0.00397\n",
      "Epoch 31: mse=0.00003   cross_entropy=0.00894   overall=0.00324   alpha=0.00410\n",
      "Epoch 32: mse=0.00003   cross_entropy=0.00895   overall=0.00306   alpha=0.00423\n",
      "Epoch 33: mse=0.00002   cross_entropy=0.00896   overall=0.00288   alpha=0.00436\n",
      "Epoch 34: mse=0.00002   cross_entropy=0.00897   overall=0.00271   alpha=0.00449\n",
      "Epoch 35: mse=0.00002   cross_entropy=0.00899   overall=0.00253   alpha=0.00462\n",
      "Epoch 36: mse=0.00002   cross_entropy=0.00900   overall=0.00235   alpha=0.00474\n",
      "Epoch 37: mse=0.00001   cross_entropy=0.00901   overall=0.00217   alpha=0.00487\n",
      "Epoch 38: mse=0.00001   cross_entropy=0.00902   overall=0.00199   alpha=0.00500\n",
      "Epoch 39: mse=0.00001   cross_entropy=0.00903   overall=0.00181   alpha=0.00513\n",
      "Epoch 40: mse=0.00001   cross_entropy=0.00904   overall=0.00163   alpha=0.00526\n",
      "Epoch 41: mse=0.00001   cross_entropy=0.00905   overall=0.00145   alpha=0.00538\n",
      "Epoch 42: mse=0.00001   cross_entropy=0.00906   overall=0.00127   alpha=0.00551\n",
      "Epoch 43: mse=0.00001   cross_entropy=0.00907   overall=0.00109   alpha=0.00564\n",
      "Epoch 44: mse=0.00000   cross_entropy=0.00908   overall=0.00091   alpha=0.00577\n",
      "Epoch 45: mse=0.00000   cross_entropy=0.00908   overall=0.00073   alpha=0.00590\n",
      "Epoch 46: mse=0.00000   cross_entropy=0.00909   overall=0.00055   alpha=0.00603\n",
      "Epoch 47: mse=0.00000   cross_entropy=0.00910   overall=0.00037   alpha=0.00615\n",
      "Epoch 48: mse=0.00000   cross_entropy=0.00911   overall=0.00019   alpha=0.00628\n",
      "Epoch 49: mse=0.00000   cross_entropy=0.00911   overall=0.00000   alpha=0.00641\n"
     ]
    }
   ],
   "source": [
    "input_size = 32\n",
    "num_teacher_logits = 6\n",
    "num_student_logits = 4\n",
    "n_epochs = 50\n",
    "\n",
    "train_loader = get_loader(num_features=input_size, num_classes=num_classes)\n",
    "teacher = Teacher(input_size, num_teacher_logits, hidden_size=10)\n",
    "student = Student(input_size, num_student_logits)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "policy = AdvancedDistillationPolicy(n_epochs, adapter_mapping=(num_teacher_logits, num_student_logits))\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec90a9",
   "metadata": {},
   "source": [
    "### Student vs Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af2bda22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher output:\n",
      "tensor([[-0.1329,  0.1282, -0.1064,  0.2148,  0.0931,  0.2372],\n",
      "        [-0.1190,  0.1222, -0.0096,  0.1823,  0.0670,  0.2634],\n",
      "        [-0.0450,  0.0447, -0.0888,  0.1382,  0.0143,  0.1679],\n",
      "        [-0.1969,  0.2049, -0.0978,  0.1848,  0.0004, -0.0018],\n",
      "        [-0.0853, -0.0314, -0.0410,  0.1831, -0.0424,  0.2571]])\n",
      "Teacher after adapter output:\n",
      "tensor([[-0.0648, -0.2931,  0.3177, -0.1903],\n",
      "        [-0.0909, -0.2952,  0.2953, -0.1913],\n",
      "        [-0.0404, -0.2404,  0.2275, -0.2135],\n",
      "        [ 0.0391, -0.2174,  0.2448, -0.2471],\n",
      "        [-0.0678, -0.2120,  0.2082, -0.2116]])\n",
      "Student output:\n",
      "tensor([[-0.0589, -0.2942,  0.3198, -0.1966],\n",
      "        [-0.0669, -0.2720,  0.2676, -0.2041],\n",
      "        [-0.0342, -0.2686,  0.2836, -0.2150],\n",
      "        [ 0.0141, -0.2429,  0.2646, -0.2340],\n",
      "        [-0.0799, -0.2555,  0.2584, -0.2100]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_input = torch.randn(5, input_size)\n",
    "    teacher_output = teacher(random_input)\n",
    "    student_output = student(random_input)\n",
    "    print('Teacher output:')\n",
    "    print(teacher_output)\n",
    "    print('Teacher output after adapter:')\n",
    "    print(policy.adapter(teacher_output))\n",
    "    print('Student output:')\n",
    "    print(student_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4b3153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00062\n"
     ]
    }
   ],
   "source": [
    "mse = nn.functional.mse_loss(student_output, policy.adapter(teacher_output)).item()\n",
    "print(f'MSE: {mse:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ebfce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
