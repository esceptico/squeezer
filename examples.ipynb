{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b779a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from squeezer.criterion import distill_loss\n",
    "from squeezer.distiller import Distiller\n",
    "from squeezer.policy import AbstractDistillationPolicy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b022dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd11587b390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0xDEAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb08840",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37483661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, n_epochs: int = 200):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        for i, (data, labels) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0d5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    preds = []\n",
    "    targets = []\n",
    "    for data, labels in loader:\n",
    "        outputs = model(data).argmax(-1)\n",
    "        preds.append(outputs)\n",
    "        targets.append(labels)\n",
    "    preds = torch.cat(preds)\n",
    "    targets = torch.cat(targets)\n",
    "    print(classification_report(targets, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d86871",
   "metadata": {},
   "source": [
    "# Models\n",
    "Объявляем модель-учитель побольше и модель-ученик поменьше.  \n",
    "Тип возвращаемого значения должен наследоваться от класса `ModelOutput` (или быть им)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69bf7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.network(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02eb47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.network(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0ea68",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8132052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(num_features: int = 64, num_classes: int = 4,\n",
    "                batch_size: int = 64, train_size: float = 0.75):\n",
    "    x, y = make_classification(\n",
    "        1000, num_features,\n",
    "        n_classes=num_classes,\n",
    "        n_informative=int(num_features * 0.9),\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=4.0,\n",
    "        random_state=0xDEAD\n",
    "    )\n",
    "    dataset = TensorDataset(\n",
    "        torch.from_numpy(x).float(),\n",
    "        torch.from_numpy(y).long()\n",
    "    )\n",
    "    dataset_length = len(x)\n",
    "    train_size = int(dataset_length * train_size)\n",
    "    val_size = dataset_length - train_size\n",
    "    train, val = random_split(dataset, [train_size, val_size])\n",
    "    return DataLoader(train, batch_size=batch_size), DataLoader(val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5bd32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 128\n",
    "num_classes = 5\n",
    "\n",
    "train_loader, val_loader = get_loaders(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b1565",
   "metadata": {},
   "source": [
    "# Train Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202d29ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 56.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        57\n",
      "           1       1.00      1.00      1.00        55\n",
      "           2       0.98      1.00      0.99        45\n",
      "           3       1.00      0.98      0.99        47\n",
      "           4       1.00      0.98      0.99        46\n",
      "\n",
      "    accuracy                           0.99       250\n",
      "   macro avg       0.99      0.99      0.99       250\n",
      "weighted avg       0.99      0.99      0.99       250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher = Teacher(num_features, num_classes, hidden_size=128)\n",
    "\n",
    "train(teacher, train_loader, n_epochs=n_epochs)\n",
    "evaluate(teacher, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29da4d9",
   "metadata": {},
   "source": [
    "# Train Student model without distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0962b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 133.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97        57\n",
      "           1       0.96      0.96      0.96        55\n",
      "           2       0.93      0.96      0.95        45\n",
      "           3       0.98      0.94      0.96        47\n",
      "           4       0.98      0.98      0.98        46\n",
      "\n",
      "    accuracy                           0.96       250\n",
      "   macro avg       0.96      0.96      0.96       250\n",
      "weighted avg       0.96      0.96      0.96       250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "train(student, train_loader, n_epochs=n_epochs)\n",
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f175ae",
   "metadata": {},
   "source": [
    "# Distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a75f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDistiller(Distiller):\n",
    "    def teacher_forward(self, batch):\n",
    "        return self.teacher(batch[0])\n",
    "    \n",
    "    def student_forward(self, batch):\n",
    "        return self.student(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b307f",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95e0ba",
   "metadata": {},
   "source": [
    "## Basic distillation policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa0427aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class BasicDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def __init__(self, temperature: float = 1.0, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, teacher_output, student_output, batch, epoch: int) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        loss_kld, loss_ce, overall = distill_loss(\n",
    "            teacher_logits=teacher_output,\n",
    "            student_logits=student_output,\n",
    "            labels=batch[1],\n",
    "            temperature=self.temperature,\n",
    "            alpha=self.alpha\n",
    "        )\n",
    "        loss_dict = {\n",
    "            'kld': loss_kld.item(),\n",
    "            'cross_entropy': loss_ce.item(),\n",
    "            'overall': overall.item(),\n",
    "        }\n",
    "        return overall, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa54f689",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.05it/s, batch loss=5.15]\n",
      "[1th epoch]: 100%|██████████| 12/12 [00:00<00:00, 366.23it/s, batch loss=4.29]\n",
      "[2th epoch]: 100%|██████████| 12/12 [00:00<00:00, 351.01it/s, batch loss=3.52]\n",
      "[3th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.62it/s, batch loss=2.88]\n",
      "[4th epoch]: 100%|██████████| 12/12 [00:00<00:00, 386.05it/s, batch loss=2.35]\n",
      "[5th epoch]: 100%|██████████| 12/12 [00:00<00:00, 415.31it/s, batch loss=1.93]\n",
      "[6th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.61it/s, batch loss=1.61]\n",
      "[7th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.88it/s, batch loss=1.35]\n",
      "[8th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.77it/s, batch loss=1.14]\n",
      "[9th epoch]: 100%|██████████| 12/12 [00:00<00:00, 422.92it/s, batch loss=0.971]\n",
      "[10th epoch]: 100%|██████████| 12/12 [00:00<00:00, 361.09it/s, batch loss=0.828]\n",
      "[11th epoch]: 100%|██████████| 12/12 [00:00<00:00, 353.03it/s, batch loss=0.707]\n",
      "[12th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.65it/s, batch loss=0.607]\n",
      "[13th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.66it/s, batch loss=0.522]\n",
      "[14th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.97it/s, batch loss=0.45]\n",
      "[15th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.36it/s, batch loss=0.389]\n",
      "[16th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.25it/s, batch loss=0.337]\n",
      "[17th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.82it/s, batch loss=0.292]\n",
      "[18th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.70it/s, batch loss=0.254]\n",
      "[19th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.31it/s, batch loss=0.221]\n",
      "[20th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.61it/s, batch loss=0.192]\n",
      "[21th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.61it/s, batch loss=0.168]\n",
      "[22th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.45it/s, batch loss=0.146]\n",
      "[23th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.99it/s, batch loss=0.128]\n",
      "[24th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.54it/s, batch loss=0.113]\n",
      "[25th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.86it/s, batch loss=0.0998]\n",
      "[26th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.83it/s, batch loss=0.0887]\n",
      "[27th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.00it/s, batch loss=0.0793]\n",
      "[28th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.91it/s, batch loss=0.0713]\n",
      "[29th epoch]: 100%|██████████| 12/12 [00:00<00:00, 379.59it/s, batch loss=0.0646]\n",
      "[30th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.22it/s, batch loss=0.0588]\n",
      "[31th epoch]: 100%|██████████| 12/12 [00:00<00:00, 403.86it/s, batch loss=0.054]\n",
      "[32th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.11it/s, batch loss=0.0498]\n",
      "[33th epoch]: 100%|██████████| 12/12 [00:00<00:00, 418.16it/s, batch loss=0.0462]\n",
      "[34th epoch]: 100%|██████████| 12/12 [00:00<00:00, 458.89it/s, batch loss=0.043]\n",
      "[35th epoch]: 100%|██████████| 12/12 [00:00<00:00, 405.07it/s, batch loss=0.0403]\n",
      "[36th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.42it/s, batch loss=0.0379]\n",
      "[37th epoch]: 100%|██████████| 12/12 [00:00<00:00, 405.83it/s, batch loss=0.0359]\n",
      "[38th epoch]: 100%|██████████| 12/12 [00:00<00:00, 371.50it/s, batch loss=0.034]\n",
      "[39th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.58it/s, batch loss=0.0324]\n",
      "[40th epoch]: 100%|██████████| 12/12 [00:00<00:00, 429.07it/s, batch loss=0.0309]\n",
      "[41th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.98it/s, batch loss=0.0296]\n",
      "[42th epoch]: 100%|██████████| 12/12 [00:00<00:00, 391.29it/s, batch loss=0.0284]\n",
      "[43th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.56it/s, batch loss=0.0274]\n",
      "[44th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.95it/s, batch loss=0.0264]\n",
      "[45th epoch]: 100%|██████████| 12/12 [00:00<00:00, 421.13it/s, batch loss=0.0256]\n",
      "[46th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.94it/s, batch loss=0.0248]\n",
      "[47th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.89it/s, batch loss=0.024]\n",
      "[48th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.05it/s, batch loss=0.0233]\n",
      "[49th epoch]: 100%|██████████| 12/12 [00:00<00:00, 450.44it/s, batch loss=0.0227]\n",
      "[50th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.30it/s, batch loss=0.0221]\n",
      "[51th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.80it/s, batch loss=0.0216]\n",
      "[52th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.93it/s, batch loss=0.0211]\n",
      "[53th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.19it/s, batch loss=0.0206]\n",
      "[54th epoch]: 100%|██████████| 12/12 [00:00<00:00, 444.67it/s, batch loss=0.0201]\n",
      "[55th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.01it/s, batch loss=0.0197]\n",
      "[56th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.67it/s, batch loss=0.0193]\n",
      "[57th epoch]: 100%|██████████| 12/12 [00:00<00:00, 453.16it/s, batch loss=0.0189]\n",
      "[58th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.02it/s, batch loss=0.0185]\n",
      "[59th epoch]: 100%|██████████| 12/12 [00:00<00:00, 378.60it/s, batch loss=0.0182]\n",
      "[60th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.47it/s, batch loss=0.0178]\n",
      "[61th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.32it/s, batch loss=0.0175]\n",
      "[62th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.11it/s, batch loss=0.0172]\n",
      "[63th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.07it/s, batch loss=0.0169]\n",
      "[64th epoch]: 100%|██████████| 12/12 [00:00<00:00, 444.43it/s, batch loss=0.0166]\n",
      "[65th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.69it/s, batch loss=0.0163]\n",
      "[66th epoch]: 100%|██████████| 12/12 [00:00<00:00, 403.85it/s, batch loss=0.016]\n",
      "[67th epoch]: 100%|██████████| 12/12 [00:00<00:00, 398.96it/s, batch loss=0.0157]\n",
      "[68th epoch]: 100%|██████████| 12/12 [00:00<00:00, 453.84it/s, batch loss=0.0155]\n",
      "[69th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.92it/s, batch loss=0.0152]\n",
      "[70th epoch]: 100%|██████████| 12/12 [00:00<00:00, 331.25it/s, batch loss=0.015]\n",
      "[71th epoch]: 100%|██████████| 12/12 [00:00<00:00, 334.08it/s, batch loss=0.0147]\n",
      "[72th epoch]: 100%|██████████| 12/12 [00:00<00:00, 329.03it/s, batch loss=0.0145]\n",
      "[73th epoch]: 100%|██████████| 12/12 [00:00<00:00, 385.02it/s, batch loss=0.0142]\n",
      "[74th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.59it/s, batch loss=0.014]\n",
      "[75th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.50it/s, batch loss=0.0138]\n",
      "[76th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.14it/s, batch loss=0.0136]\n",
      "[77th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.01it/s, batch loss=0.0133]\n",
      "[78th epoch]: 100%|██████████| 12/12 [00:00<00:00, 452.37it/s, batch loss=0.0131]\n",
      "[79th epoch]: 100%|██████████| 12/12 [00:00<00:00, 451.23it/s, batch loss=0.0129]\n",
      "[80th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.86it/s, batch loss=0.0127]\n",
      "[81th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.14it/s, batch loss=0.0125]\n",
      "[82th epoch]: 100%|██████████| 12/12 [00:00<00:00, 447.75it/s, batch loss=0.0123]\n",
      "[83th epoch]: 100%|██████████| 12/12 [00:00<00:00, 416.81it/s, batch loss=0.0121]\n",
      "[84th epoch]: 100%|██████████| 12/12 [00:00<00:00, 455.81it/s, batch loss=0.0119]\n",
      "[85th epoch]: 100%|██████████| 12/12 [00:00<00:00, 448.01it/s, batch loss=0.0117]\n",
      "[86th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.69it/s, batch loss=0.0115]\n",
      "[87th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.53it/s, batch loss=0.0113]\n",
      "[88th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.96it/s, batch loss=0.0112]\n",
      "[89th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.27it/s, batch loss=0.011]\n",
      "[90th epoch]: 100%|██████████| 12/12 [00:00<00:00, 433.84it/s, batch loss=0.0108]\n",
      "[91th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.15it/s, batch loss=0.0106]\n",
      "[92th epoch]: 100%|██████████| 12/12 [00:00<00:00, 453.34it/s, batch loss=0.0105]\n",
      "[93th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.88it/s, batch loss=0.0103]\n",
      "[94th epoch]: 100%|██████████| 12/12 [00:00<00:00, 450.08it/s, batch loss=0.0102]\n",
      "[95th epoch]: 100%|██████████| 12/12 [00:00<00:00, 366.26it/s, batch loss=0.01]\n",
      "[96th epoch]: 100%|██████████| 12/12 [00:00<00:00, 327.69it/s, batch loss=0.00986]\n",
      "[97th epoch]: 100%|██████████| 12/12 [00:00<00:00, 348.75it/s, batch loss=0.00971]\n",
      "[98th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.65it/s, batch loss=0.00956]\n",
      "[99th epoch]: 100%|██████████| 12/12 [00:00<00:00, 426.08it/s, batch loss=0.00942]\n",
      "[100th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.76it/s, batch loss=0.00928]\n",
      "[101th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.87it/s, batch loss=0.00914]\n",
      "[102th epoch]: 100%|██████████| 12/12 [00:00<00:00, 429.76it/s, batch loss=0.00901]\n",
      "[103th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.60it/s, batch loss=0.00888]\n",
      "[104th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.83it/s, batch loss=0.00875]\n",
      "[105th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.52it/s, batch loss=0.00862]\n",
      "[106th epoch]: 100%|██████████| 12/12 [00:00<00:00, 455.46it/s, batch loss=0.0085]\n",
      "[107th epoch]: 100%|██████████| 12/12 [00:00<00:00, 464.33it/s, batch loss=0.00838]\n",
      "[108th epoch]: 100%|██████████| 12/12 [00:00<00:00, 455.91it/s, batch loss=0.00826]\n",
      "[109th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.37it/s, batch loss=0.00815]\n",
      "[110th epoch]: 100%|██████████| 12/12 [00:00<00:00, 452.03it/s, batch loss=0.00803]\n",
      "[111th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.72it/s, batch loss=0.00792]\n",
      "[112th epoch]: 100%|██████████| 12/12 [00:00<00:00, 447.08it/s, batch loss=0.00781]\n",
      "[113th epoch]: 100%|██████████| 12/12 [00:00<00:00, 459.17it/s, batch loss=0.00771]\n",
      "[114th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.90it/s, batch loss=0.0076]\n",
      "[115th epoch]: 100%|██████████| 12/12 [00:00<00:00, 463.14it/s, batch loss=0.0075]\n",
      "[116th epoch]: 100%|██████████| 12/12 [00:00<00:00, 413.64it/s, batch loss=0.0074]\n",
      "[117th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.12it/s, batch loss=0.0073]\n",
      "[118th epoch]: 100%|██████████| 12/12 [00:00<00:00, 464.04it/s, batch loss=0.00721]\n",
      "[119th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.26it/s, batch loss=0.00711]\n",
      "[120th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.20it/s, batch loss=0.00702]\n",
      "[121th epoch]: 100%|██████████| 12/12 [00:00<00:00, 467.28it/s, batch loss=0.00693]\n",
      "[122th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.51it/s, batch loss=0.00684]\n",
      "[123th epoch]: 100%|██████████| 12/12 [00:00<00:00, 388.95it/s, batch loss=0.00675]\n",
      "[124th epoch]: 100%|██████████| 12/12 [00:00<00:00, 433.56it/s, batch loss=0.00666]\n",
      "[125th epoch]: 100%|██████████| 12/12 [00:00<00:00, 384.16it/s, batch loss=0.00658]\n",
      "[126th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.67it/s, batch loss=0.0065]\n",
      "[127th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.34it/s, batch loss=0.00642]\n",
      "[128th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.68it/s, batch loss=0.00634]\n",
      "[129th epoch]: 100%|██████████| 12/12 [00:00<00:00, 448.06it/s, batch loss=0.00626]\n",
      "[130th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.61it/s, batch loss=0.00618]\n",
      "[131th epoch]: 100%|██████████| 12/12 [00:00<00:00, 466.38it/s, batch loss=0.0061]\n",
      "[132th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.22it/s, batch loss=0.00603]\n",
      "[133th epoch]: 100%|██████████| 12/12 [00:00<00:00, 452.18it/s, batch loss=0.00596]\n",
      "[134th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.00it/s, batch loss=0.00589]\n",
      "[135th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.96it/s, batch loss=0.00582]\n",
      "[136th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.60it/s, batch loss=0.00575]\n",
      "[137th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.26it/s, batch loss=0.00568]\n",
      "[138th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.53it/s, batch loss=0.00561]\n",
      "[139th epoch]: 100%|██████████| 12/12 [00:00<00:00, 379.72it/s, batch loss=0.00555]\n",
      "[140th epoch]: 100%|██████████| 12/12 [00:00<00:00, 462.71it/s, batch loss=0.00548]\n",
      "[141th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.18it/s, batch loss=0.00542]\n",
      "[142th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.13it/s, batch loss=0.00536]\n",
      "[143th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.97it/s, batch loss=0.00529]\n",
      "[144th epoch]: 100%|██████████| 12/12 [00:00<00:00, 471.80it/s, batch loss=0.00523]\n",
      "[145th epoch]: 100%|██████████| 12/12 [00:00<00:00, 451.64it/s, batch loss=0.00517]\n",
      "[146th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.71it/s, batch loss=0.00512]\n",
      "[147th epoch]: 100%|██████████| 12/12 [00:00<00:00, 478.17it/s, batch loss=0.00506]\n",
      "[148th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.28it/s, batch loss=0.005]\n",
      "[149th epoch]: 100%|██████████| 12/12 [00:00<00:00, 461.90it/s, batch loss=0.00495]\n",
      "[150th epoch]: 100%|██████████| 12/12 [00:00<00:00, 451.98it/s, batch loss=0.00489]\n",
      "[151th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.12it/s, batch loss=0.00484]\n",
      "[152th epoch]: 100%|██████████| 12/12 [00:00<00:00, 416.36it/s, batch loss=0.00479]\n",
      "[153th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.27it/s, batch loss=0.00473]\n",
      "[154th epoch]: 100%|██████████| 12/12 [00:00<00:00, 451.69it/s, batch loss=0.00468]\n",
      "[155th epoch]: 100%|██████████| 12/12 [00:00<00:00, 470.22it/s, batch loss=0.00463]\n",
      "[156th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.05it/s, batch loss=0.00458]\n",
      "[157th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.81it/s, batch loss=0.00453]\n",
      "[158th epoch]: 100%|██████████| 12/12 [00:00<00:00, 459.34it/s, batch loss=0.00448]\n",
      "[159th epoch]: 100%|██████████| 12/12 [00:00<00:00, 461.14it/s, batch loss=0.00444]\n",
      "[160th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.89it/s, batch loss=0.00439]\n",
      "[161th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.82it/s, batch loss=0.00434]\n",
      "[162th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.23it/s, batch loss=0.0043]\n",
      "[163th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.42it/s, batch loss=0.00425]\n",
      "[164th epoch]: 100%|██████████| 12/12 [00:00<00:00, 459.88it/s, batch loss=0.00421]\n",
      "[165th epoch]: 100%|██████████| 12/12 [00:00<00:00, 455.46it/s, batch loss=0.00417]\n",
      "[166th epoch]: 100%|██████████| 12/12 [00:00<00:00, 472.31it/s, batch loss=0.00412]\n",
      "[167th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.27it/s, batch loss=0.00408]\n",
      "[168th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.84it/s, batch loss=0.00404]\n",
      "[169th epoch]: 100%|██████████| 12/12 [00:00<00:00, 472.01it/s, batch loss=0.004]\n",
      "[170th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.42it/s, batch loss=0.00396]\n",
      "[171th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.48it/s, batch loss=0.00392]\n",
      "[172th epoch]: 100%|██████████| 12/12 [00:00<00:00, 472.09it/s, batch loss=0.00388]\n",
      "[173th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.09it/s, batch loss=0.00384]\n",
      "[174th epoch]: 100%|██████████| 12/12 [00:00<00:00, 455.75it/s, batch loss=0.0038]\n",
      "[175th epoch]: 100%|██████████| 12/12 [00:00<00:00, 452.03it/s, batch loss=0.00377]\n",
      "[176th epoch]: 100%|██████████| 12/12 [00:00<00:00, 447.60it/s, batch loss=0.00373]\n",
      "[177th epoch]: 100%|██████████| 12/12 [00:00<00:00, 471.16it/s, batch loss=0.00369]\n",
      "[178th epoch]: 100%|██████████| 12/12 [00:00<00:00, 472.27it/s, batch loss=0.00366]\n",
      "[179th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.44it/s, batch loss=0.00362]\n",
      "[180th epoch]: 100%|██████████| 12/12 [00:00<00:00, 460.25it/s, batch loss=0.00359]\n",
      "[181th epoch]: 100%|██████████| 12/12 [00:00<00:00, 467.11it/s, batch loss=0.00355]\n",
      "[182th epoch]: 100%|██████████| 12/12 [00:00<00:00, 462.63it/s, batch loss=0.00352]\n",
      "[183th epoch]: 100%|██████████| 12/12 [00:00<00:00, 447.99it/s, batch loss=0.00349]\n",
      "[184th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.62it/s, batch loss=0.00345]\n",
      "[185th epoch]: 100%|██████████| 12/12 [00:00<00:00, 479.23it/s, batch loss=0.00342]\n",
      "[186th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.56it/s, batch loss=0.00339]\n",
      "[187th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.30it/s, batch loss=0.00336]\n",
      "[188th epoch]: 100%|██████████| 12/12 [00:00<00:00, 452.85it/s, batch loss=0.00332]\n",
      "[189th epoch]: 100%|██████████| 12/12 [00:00<00:00, 478.70it/s, batch loss=0.00329]\n",
      "[190th epoch]: 100%|██████████| 12/12 [00:00<00:00, 469.52it/s, batch loss=0.00326]\n",
      "[191th epoch]: 100%|██████████| 12/12 [00:00<00:00, 458.01it/s, batch loss=0.00323]\n",
      "[192th epoch]: 100%|██████████| 12/12 [00:00<00:00, 460.26it/s, batch loss=0.0032]\n",
      "[193th epoch]: 100%|██████████| 12/12 [00:00<00:00, 468.22it/s, batch loss=0.00317]\n",
      "[194th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.63it/s, batch loss=0.00314]\n",
      "[195th epoch]: 100%|██████████| 12/12 [00:00<00:00, 460.34it/s, batch loss=0.00312]\n",
      "[196th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.97it/s, batch loss=0.00309]\n",
      "[197th epoch]: 100%|██████████| 12/12 [00:00<00:00, 461.52it/s, batch loss=0.00306]\n",
      "[198th epoch]: 100%|██████████| 12/12 [00:00<00:00, 452.76it/s, batch loss=0.00303]\n",
      "[199th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.45it/s, batch loss=0.003]\n",
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "# Инициализируем политику функции потерь для дистилляции.\n",
    "# В этом примере используется стандартная политика, при которой\n",
    "# ученик учится сразу на распределение логитов учителя (KLD) и на мейнстрим задачу (CE).\n",
    "# Для кастомизации политики, например, для использования других функций потерь или\n",
    "# добавления адаптеров между аутпутами моделей, необходимо наследоваться от класса `AbstractDistillationPolicy`\n",
    "policy = BasicDistillationPolicy(temperature=1.2, alpha=0.5)\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, val_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342a14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97        57\n",
      "           1       1.00      0.96      0.98        55\n",
      "           2       0.96      1.00      0.98        45\n",
      "           3       1.00      0.89      0.94        47\n",
      "           4       0.96      0.98      0.97        46\n",
      "\n",
      "    accuracy                           0.97       250\n",
      "   macro avg       0.97      0.97      0.97       250\n",
      "weighted avg       0.97      0.97      0.97       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a573fb95",
   "metadata": {},
   "source": [
    "## Custom policy (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25660d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class SingleMSEDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def forward(self, teacher_output, student_output, batch, epoch) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        loss_mse = nn.functional.mse_loss(student_output, teacher_output)\n",
    "        loss_dict = {'mse': loss_mse.item()}\n",
    "        return loss_mse, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60d9d511",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0th epoch]: 100%|██████████| 12/12 [00:00<00:00, 310.41it/s, loss=78.7]\n",
      "[1th epoch]: 100%|██████████| 12/12 [00:00<00:00, 285.68it/s, loss=71.5]\n",
      "[2th epoch]: 100%|██████████| 12/12 [00:00<00:00, 357.37it/s, loss=65]\n",
      "[3th epoch]: 100%|██████████| 12/12 [00:00<00:00, 357.59it/s, loss=59]\n",
      "[4th epoch]: 100%|██████████| 12/12 [00:00<00:00, 359.22it/s, loss=53.6]\n",
      "[5th epoch]: 100%|██████████| 12/12 [00:00<00:00, 224.45it/s, loss=48.8]\n",
      "[6th epoch]: 100%|██████████| 12/12 [00:00<00:00, 401.40it/s, loss=44.4]\n",
      "[7th epoch]: 100%|██████████| 12/12 [00:00<00:00, 341.66it/s, loss=40.5]\n",
      "[8th epoch]: 100%|██████████| 12/12 [00:00<00:00, 341.18it/s, loss=36.9]\n",
      "[9th epoch]: 100%|██████████| 12/12 [00:00<00:00, 317.45it/s, loss=33.8]\n",
      "[10th epoch]: 100%|██████████| 12/12 [00:00<00:00, 354.54it/s, loss=30.9]\n",
      "[11th epoch]: 100%|██████████| 12/12 [00:00<00:00, 331.88it/s, loss=28.4]\n",
      "[12th epoch]: 100%|██████████| 12/12 [00:00<00:00, 355.72it/s, loss=26.1]\n",
      "[13th epoch]: 100%|██████████| 12/12 [00:00<00:00, 336.33it/s, loss=24.1]\n",
      "[14th epoch]: 100%|██████████| 12/12 [00:00<00:00, 381.49it/s, loss=22.2]\n",
      "[15th epoch]: 100%|██████████| 12/12 [00:00<00:00, 410.79it/s, loss=20.6]\n",
      "[16th epoch]: 100%|██████████| 12/12 [00:00<00:00, 422.06it/s, loss=19.1]\n",
      "[17th epoch]: 100%|██████████| 12/12 [00:00<00:00, 357.66it/s, loss=17.8]\n",
      "[18th epoch]: 100%|██████████| 12/12 [00:00<00:00, 301.38it/s, loss=16.7]\n",
      "[19th epoch]: 100%|██████████| 12/12 [00:00<00:00, 295.76it/s, loss=15.6]\n",
      "[20th epoch]: 100%|██████████| 12/12 [00:00<00:00, 323.42it/s, loss=14.7]\n",
      "[21th epoch]: 100%|██████████| 12/12 [00:00<00:00, 384.52it/s, loss=13.8]\n",
      "[22th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.22it/s, loss=13.1]\n",
      "[23th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.40it/s, loss=12.4]\n",
      "[24th epoch]: 100%|██████████| 12/12 [00:00<00:00, 376.59it/s, loss=11.8]\n",
      "[25th epoch]: 100%|██████████| 12/12 [00:00<00:00, 389.02it/s, loss=11.2]\n",
      "[26th epoch]: 100%|██████████| 12/12 [00:00<00:00, 338.11it/s, loss=10.7]\n",
      "[27th epoch]: 100%|██████████| 12/12 [00:00<00:00, 312.81it/s, loss=10.2]\n",
      "[28th epoch]: 100%|██████████| 12/12 [00:00<00:00, 347.85it/s, loss=9.79]\n",
      "[29th epoch]: 100%|██████████| 12/12 [00:00<00:00, 394.08it/s, loss=9.4]\n",
      "[30th epoch]: 100%|██████████| 12/12 [00:00<00:00, 363.28it/s, loss=9.04]\n",
      "[31th epoch]: 100%|██████████| 12/12 [00:00<00:00, 367.35it/s, loss=8.7]\n",
      "[32th epoch]: 100%|██████████| 12/12 [00:00<00:00, 375.29it/s, loss=8.4]\n",
      "[33th epoch]: 100%|██████████| 12/12 [00:00<00:00, 382.64it/s, loss=8.11]\n",
      "[34th epoch]: 100%|██████████| 12/12 [00:00<00:00, 356.01it/s, loss=7.85]\n",
      "[35th epoch]: 100%|██████████| 12/12 [00:00<00:00, 359.54it/s, loss=7.6]\n",
      "[36th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.84it/s, loss=7.36]\n",
      "[37th epoch]: 100%|██████████| 12/12 [00:00<00:00, 277.91it/s, loss=7.15]\n",
      "[38th epoch]: 100%|██████████| 12/12 [00:00<00:00, 282.13it/s, loss=6.94]\n",
      "[39th epoch]: 100%|██████████| 12/12 [00:00<00:00, 333.21it/s, loss=6.75]\n",
      "[40th epoch]: 100%|██████████| 12/12 [00:00<00:00, 324.69it/s, loss=6.57]\n",
      "[41th epoch]: 100%|██████████| 12/12 [00:00<00:00, 337.82it/s, loss=6.39]\n",
      "[42th epoch]: 100%|██████████| 12/12 [00:00<00:00, 341.81it/s, loss=6.23]\n",
      "[43th epoch]: 100%|██████████| 12/12 [00:00<00:00, 317.70it/s, loss=6.08]\n",
      "[44th epoch]: 100%|██████████| 12/12 [00:00<00:00, 287.58it/s, loss=5.93]\n",
      "[45th epoch]: 100%|██████████| 12/12 [00:00<00:00, 339.74it/s, loss=5.79]\n",
      "[46th epoch]: 100%|██████████| 12/12 [00:00<00:00, 359.12it/s, loss=5.65]\n",
      "[47th epoch]: 100%|██████████| 12/12 [00:00<00:00, 333.67it/s, loss=5.53]\n",
      "[48th epoch]: 100%|██████████| 12/12 [00:00<00:00, 324.84it/s, loss=5.4]\n",
      "[49th epoch]: 100%|██████████| 12/12 [00:00<00:00, 354.63it/s, loss=5.29]\n",
      "[50th epoch]: 100%|██████████| 12/12 [00:00<00:00, 319.28it/s, loss=5.18]\n",
      "[51th epoch]: 100%|██████████| 12/12 [00:00<00:00, 321.76it/s, loss=5.07]\n",
      "[52th epoch]: 100%|██████████| 12/12 [00:00<00:00, 336.90it/s, loss=4.97]\n",
      "[53th epoch]: 100%|██████████| 12/12 [00:00<00:00, 328.49it/s, loss=4.87]\n",
      "[54th epoch]: 100%|██████████| 12/12 [00:00<00:00, 338.08it/s, loss=4.78]\n",
      "[55th epoch]: 100%|██████████| 12/12 [00:00<00:00, 369.71it/s, loss=4.69]\n",
      "[56th epoch]: 100%|██████████| 12/12 [00:00<00:00, 405.45it/s, loss=4.61]\n",
      "[57th epoch]: 100%|██████████| 12/12 [00:00<00:00, 387.36it/s, loss=4.52]\n",
      "[58th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.61it/s, loss=4.45]\n",
      "[59th epoch]: 100%|██████████| 12/12 [00:00<00:00, 389.32it/s, loss=4.37]\n",
      "[60th epoch]: 100%|██████████| 12/12 [00:00<00:00, 413.04it/s, loss=4.3]\n",
      "[61th epoch]: 100%|██████████| 12/12 [00:00<00:00, 376.51it/s, loss=4.23]\n",
      "[62th epoch]: 100%|██████████| 12/12 [00:00<00:00, 392.24it/s, loss=4.17]\n",
      "[63th epoch]: 100%|██████████| 12/12 [00:00<00:00, 416.93it/s, loss=4.1]\n",
      "[64th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.89it/s, loss=4.04]\n",
      "[65th epoch]: 100%|██████████| 12/12 [00:00<00:00, 390.81it/s, loss=3.98]\n",
      "[66th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.03it/s, loss=3.93]\n",
      "[67th epoch]: 100%|██████████| 12/12 [00:00<00:00, 395.67it/s, loss=3.87]\n",
      "[68th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.57it/s, loss=3.82]\n",
      "[69th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.33it/s, loss=3.77]\n",
      "[70th epoch]: 100%|██████████| 12/12 [00:00<00:00, 390.53it/s, loss=3.73]\n",
      "[71th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.12it/s, loss=3.68]\n",
      "[72th epoch]: 100%|██████████| 12/12 [00:00<00:00, 390.94it/s, loss=3.64]\n",
      "[73th epoch]: 100%|██████████| 12/12 [00:00<00:00, 395.81it/s, loss=3.6]\n",
      "[74th epoch]: 100%|██████████| 12/12 [00:00<00:00, 391.14it/s, loss=3.56]\n",
      "[75th epoch]: 100%|██████████| 12/12 [00:00<00:00, 386.68it/s, loss=3.52]\n",
      "[76th epoch]: 100%|██████████| 12/12 [00:00<00:00, 401.26it/s, loss=3.48]\n",
      "[77th epoch]: 100%|██████████| 12/12 [00:00<00:00, 382.69it/s, loss=3.44]\n",
      "[78th epoch]: 100%|██████████| 12/12 [00:00<00:00, 390.70it/s, loss=3.41]\n",
      "[79th epoch]: 100%|██████████| 12/12 [00:00<00:00, 370.58it/s, loss=3.38]\n",
      "[80th epoch]: 100%|██████████| 12/12 [00:00<00:00, 381.97it/s, loss=3.35]\n",
      "[81th epoch]: 100%|██████████| 12/12 [00:00<00:00, 369.52it/s, loss=3.32]\n",
      "[82th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.22it/s, loss=3.29]\n",
      "[83th epoch]: 100%|██████████| 12/12 [00:00<00:00, 401.90it/s, loss=3.26]\n",
      "[84th epoch]: 100%|██████████| 12/12 [00:00<00:00, 392.62it/s, loss=3.23]\n",
      "[85th epoch]: 100%|██████████| 12/12 [00:00<00:00, 361.58it/s, loss=3.21]\n",
      "[86th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.45it/s, loss=3.18]\n",
      "[87th epoch]: 100%|██████████| 12/12 [00:00<00:00, 403.39it/s, loss=3.16]\n",
      "[88th epoch]: 100%|██████████| 12/12 [00:00<00:00, 386.60it/s, loss=3.13]\n",
      "[89th epoch]: 100%|██████████| 12/12 [00:00<00:00, 375.14it/s, loss=3.11]\n",
      "[90th epoch]: 100%|██████████| 12/12 [00:00<00:00, 390.71it/s, loss=3.09]\n",
      "[91th epoch]: 100%|██████████| 12/12 [00:00<00:00, 366.10it/s, loss=3.07]\n",
      "[92th epoch]: 100%|██████████| 12/12 [00:00<00:00, 413.41it/s, loss=3.05]\n",
      "[93th epoch]: 100%|██████████| 12/12 [00:00<00:00, 415.94it/s, loss=3.03]\n",
      "[94th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.73it/s, loss=3.01]\n",
      "[95th epoch]: 100%|██████████| 12/12 [00:00<00:00, 377.27it/s, loss=3]\n",
      "[96th epoch]: 100%|██████████| 12/12 [00:00<00:00, 425.97it/s, loss=2.98]\n",
      "[97th epoch]: 100%|██████████| 12/12 [00:00<00:00, 404.57it/s, loss=2.96]\n",
      "[98th epoch]: 100%|██████████| 12/12 [00:00<00:00, 413.14it/s, loss=2.95]\n",
      "[99th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.09it/s, loss=2.93]\n",
      "[100th epoch]: 100%|██████████| 12/12 [00:00<00:00, 382.20it/s, loss=2.92]\n",
      "[101th epoch]: 100%|██████████| 12/12 [00:00<00:00, 418.79it/s, loss=2.9]\n",
      "[102th epoch]: 100%|██████████| 12/12 [00:00<00:00, 425.05it/s, loss=2.89]\n",
      "[103th epoch]: 100%|██████████| 12/12 [00:00<00:00, 376.49it/s, loss=2.88]\n",
      "[104th epoch]: 100%|██████████| 12/12 [00:00<00:00, 391.72it/s, loss=2.87]\n",
      "[105th epoch]: 100%|██████████| 12/12 [00:00<00:00, 405.42it/s, loss=2.85]\n",
      "[106th epoch]: 100%|██████████| 12/12 [00:00<00:00, 394.13it/s, loss=2.84]\n",
      "[107th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.58it/s, loss=2.83]\n",
      "[108th epoch]: 100%|██████████| 12/12 [00:00<00:00, 389.24it/s, loss=2.82]\n",
      "[109th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.78it/s, loss=2.81]\n",
      "[110th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.62it/s, loss=2.8]\n",
      "[111th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.69it/s, loss=2.79]\n",
      "[112th epoch]: 100%|██████████| 12/12 [00:00<00:00, 432.57it/s, loss=2.78]\n",
      "[113th epoch]: 100%|██████████| 12/12 [00:00<00:00, 398.15it/s, loss=2.77]\n",
      "[114th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.17it/s, loss=2.77]\n",
      "[115th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.91it/s, loss=2.76]\n",
      "[116th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.62it/s, loss=2.75]\n",
      "[117th epoch]: 100%|██████████| 12/12 [00:00<00:00, 395.15it/s, loss=2.74]\n",
      "[118th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.79it/s, loss=2.74]\n",
      "[119th epoch]: 100%|██████████| 12/12 [00:00<00:00, 413.15it/s, loss=2.73]\n",
      "[120th epoch]: 100%|██████████| 12/12 [00:00<00:00, 398.88it/s, loss=2.72]\n",
      "[121th epoch]: 100%|██████████| 12/12 [00:00<00:00, 367.75it/s, loss=2.72]\n",
      "[122th epoch]: 100%|██████████| 12/12 [00:00<00:00, 370.09it/s, loss=2.71]\n",
      "[123th epoch]: 100%|██████████| 12/12 [00:00<00:00, 403.59it/s, loss=2.7]\n",
      "[124th epoch]: 100%|██████████| 12/12 [00:00<00:00, 398.01it/s, loss=2.7]\n",
      "[125th epoch]: 100%|██████████| 12/12 [00:00<00:00, 365.07it/s, loss=2.69]\n",
      "[126th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.05it/s, loss=2.69]\n",
      "[127th epoch]: 100%|██████████| 12/12 [00:00<00:00, 395.60it/s, loss=2.68]\n",
      "[128th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.64it/s, loss=2.68]\n",
      "[129th epoch]: 100%|██████████| 12/12 [00:00<00:00, 455.84it/s, loss=2.67]\n",
      "[130th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.67it/s, loss=2.67]\n",
      "[131th epoch]: 100%|██████████| 12/12 [00:00<00:00, 444.69it/s, loss=2.66]\n",
      "[132th epoch]: 100%|██████████| 12/12 [00:00<00:00, 389.70it/s, loss=2.66]\n",
      "[133th epoch]: 100%|██████████| 12/12 [00:00<00:00, 394.98it/s, loss=2.65]\n",
      "[134th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.56it/s, loss=2.65]\n",
      "[135th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.67it/s, loss=2.65]\n",
      "[136th epoch]: 100%|██████████| 12/12 [00:00<00:00, 385.97it/s, loss=2.64]\n",
      "[137th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.70it/s, loss=2.64]\n",
      "[138th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.37it/s, loss=2.63]\n",
      "[139th epoch]: 100%|██████████| 12/12 [00:00<00:00, 398.76it/s, loss=2.63]\n",
      "[140th epoch]: 100%|██████████| 12/12 [00:00<00:00, 447.76it/s, loss=2.63]\n",
      "[141th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.34it/s, loss=2.63]\n",
      "[142th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.85it/s, loss=2.62]\n",
      "[143th epoch]: 100%|██████████| 12/12 [00:00<00:00, 451.83it/s, loss=2.62]\n",
      "[144th epoch]: 100%|██████████| 12/12 [00:00<00:00, 461.59it/s, loss=2.62]\n",
      "[145th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.09it/s, loss=2.61]\n",
      "[146th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.25it/s, loss=2.61]\n",
      "[147th epoch]: 100%|██████████| 12/12 [00:00<00:00, 417.07it/s, loss=2.61]\n",
      "[148th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.38it/s, loss=2.61]\n",
      "[149th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.23it/s, loss=2.6]\n",
      "[150th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.41it/s, loss=2.6]\n",
      "[151th epoch]: 100%|██████████| 12/12 [00:00<00:00, 388.40it/s, loss=2.6]\n",
      "[152th epoch]: 100%|██████████| 12/12 [00:00<00:00, 417.75it/s, loss=2.6]\n",
      "[153th epoch]: 100%|██████████| 12/12 [00:00<00:00, 422.81it/s, loss=2.59]\n",
      "[154th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.47it/s, loss=2.59]\n",
      "[155th epoch]: 100%|██████████| 12/12 [00:00<00:00, 387.91it/s, loss=2.59]\n",
      "[156th epoch]: 100%|██████████| 12/12 [00:00<00:00, 390.45it/s, loss=2.59]\n",
      "[157th epoch]: 100%|██████████| 12/12 [00:00<00:00, 425.35it/s, loss=2.59]\n",
      "[158th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.94it/s, loss=2.59]\n",
      "[159th epoch]: 100%|██████████| 12/12 [00:00<00:00, 407.88it/s, loss=2.58]\n",
      "[160th epoch]: 100%|██████████| 12/12 [00:00<00:00, 422.33it/s, loss=2.58]\n",
      "[161th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.27it/s, loss=2.58]\n",
      "[162th epoch]: 100%|██████████| 12/12 [00:00<00:00, 417.38it/s, loss=2.58]\n",
      "[163th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.63it/s, loss=2.58]\n",
      "[164th epoch]: 100%|██████████| 12/12 [00:00<00:00, 386.09it/s, loss=2.58]\n",
      "[165th epoch]: 100%|██████████| 12/12 [00:00<00:00, 447.64it/s, loss=2.57]\n",
      "[166th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.37it/s, loss=2.57]\n",
      "[167th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.02it/s, loss=2.57]\n",
      "[168th epoch]: 100%|██████████| 12/12 [00:00<00:00, 399.65it/s, loss=2.57]\n",
      "[169th epoch]: 100%|██████████| 12/12 [00:00<00:00, 404.74it/s, loss=2.57]\n",
      "[170th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.13it/s, loss=2.57]\n",
      "[171th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.25it/s, loss=2.57]\n",
      "[172th epoch]: 100%|██████████| 12/12 [00:00<00:00, 388.60it/s, loss=2.57]\n",
      "[173th epoch]: 100%|██████████| 12/12 [00:00<00:00, 350.30it/s, loss=2.56]\n",
      "[174th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.16it/s, loss=2.56]\n",
      "[175th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.49it/s, loss=2.56]\n",
      "[176th epoch]: 100%|██████████| 12/12 [00:00<00:00, 444.68it/s, loss=2.56]\n",
      "[177th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.20it/s, loss=2.56]\n",
      "[178th epoch]: 100%|██████████| 12/12 [00:00<00:00, 381.80it/s, loss=2.56]\n",
      "[179th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.71it/s, loss=2.56]\n",
      "[180th epoch]: 100%|██████████| 12/12 [00:00<00:00, 421.42it/s, loss=2.56]\n",
      "[181th epoch]: 100%|██████████| 12/12 [00:00<00:00, 462.66it/s, loss=2.56]\n",
      "[182th epoch]: 100%|██████████| 12/12 [00:00<00:00, 405.82it/s, loss=2.56]\n",
      "[183th epoch]: 100%|██████████| 12/12 [00:00<00:00, 401.94it/s, loss=2.55]\n",
      "[184th epoch]: 100%|██████████| 12/12 [00:00<00:00, 432.40it/s, loss=2.55]\n",
      "[185th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.17it/s, loss=2.55]\n",
      "[186th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.13it/s, loss=2.55]\n",
      "[187th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.87it/s, loss=2.55]\n",
      "[188th epoch]: 100%|██████████| 12/12 [00:00<00:00, 399.11it/s, loss=2.55]\n",
      "[189th epoch]: 100%|██████████| 12/12 [00:00<00:00, 407.25it/s, loss=2.55]\n",
      "[190th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.96it/s, loss=2.55]\n",
      "[191th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.67it/s, loss=2.55]\n",
      "[192th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.66it/s, loss=2.55]\n",
      "[193th epoch]: 100%|██████████| 12/12 [00:00<00:00, 358.13it/s, loss=2.55]\n",
      "[194th epoch]: 100%|██████████| 12/12 [00:00<00:00, 335.36it/s, loss=2.55]\n",
      "[195th epoch]: 100%|██████████| 12/12 [00:00<00:00, 392.39it/s, loss=2.55]\n",
      "[196th epoch]: 100%|██████████| 12/12 [00:00<00:00, 429.33it/s, loss=2.55]\n",
      "[197th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.64it/s, loss=2.54]\n",
      "[198th epoch]: 100%|██████████| 12/12 [00:00<00:00, 416.97it/s, loss=2.54]\n",
      "[199th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.09it/s, loss=2.54]\n",
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "policy = SingleMSEDistillationPolicy()\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, val_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5738b022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        57\n",
      "           1       1.00      1.00      1.00        55\n",
      "           2       0.98      1.00      0.99        45\n",
      "           3       1.00      0.98      0.99        47\n",
      "           4       1.00      0.98      0.99        46\n",
      "\n",
      "    accuracy                           0.99       250\n",
      "   macro avg       0.99      0.99      0.99       250\n",
      "weighted avg       0.99      0.99      0.99       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17679a4",
   "metadata": {},
   "source": [
    "## Advanced policy\n",
    "**CE + MSE with scale decay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2043a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd1c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossDictT = Dict[str, float]\n",
    "\n",
    "\n",
    "class AdvancedDistillationPolicy(AbstractDistillationPolicy):\n",
    "    def __init__(self, n_epochs: int, adapter_mapping: Optional[Tuple[int, int]] = None):\n",
    "        super().__init__()\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def forward(self, teacher_output, student_output, batch, epoch) -> Tuple[torch.Tensor, LossDictT]:\n",
    "        alpha = math.exp((epoch + 1) / self.n_epochs)\n",
    "        loss_mse = nn.functional.mse_loss(student_output, teacher_output)\n",
    "        loss_ce = nn.functional.cross_entropy(student_output, batch[1])\n",
    "        overall = loss_mse * alpha + loss_ce * (1 - alpha)\n",
    "        scalars_dict = {\n",
    "            'mse': loss_mse.item(),\n",
    "            'cross_entropy': loss_ce.item(),\n",
    "            'overall': overall.item(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "        return overall, scalars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e2964d2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.17it/s, batch loss=73.5]\n",
      "[1th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.34it/s, batch loss=67.3]\n",
      "[2th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.28it/s, batch loss=61.7]\n",
      "[3th epoch]: 100%|██████████| 12/12 [00:00<00:00, 343.67it/s, batch loss=56.4]\n",
      "[4th epoch]: 100%|██████████| 12/12 [00:00<00:00, 358.42it/s, batch loss=51.7]\n",
      "[5th epoch]: 100%|██████████| 12/12 [00:00<00:00, 397.20it/s, batch loss=47.4]\n",
      "[6th epoch]: 100%|██████████| 12/12 [00:00<00:00, 397.02it/s, batch loss=43.5]\n",
      "[7th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.60it/s, batch loss=40]\n",
      "[8th epoch]: 100%|██████████| 12/12 [00:00<00:00, 335.31it/s, batch loss=36.8]\n",
      "[9th epoch]: 100%|██████████| 12/12 [00:00<00:00, 381.54it/s, batch loss=33.9]\n",
      "[10th epoch]: 100%|██████████| 12/12 [00:00<00:00, 407.07it/s, batch loss=31.3]\n",
      "[11th epoch]: 100%|██████████| 12/12 [00:00<00:00, 359.76it/s, batch loss=29]\n",
      "[12th epoch]: 100%|██████████| 12/12 [00:00<00:00, 410.82it/s, batch loss=26.9]\n",
      "[13th epoch]: 100%|██████████| 12/12 [00:00<00:00, 410.92it/s, batch loss=25]\n",
      "[14th epoch]: 100%|██████████| 12/12 [00:00<00:00, 379.73it/s, batch loss=23.3]\n",
      "[15th epoch]: 100%|██████████| 12/12 [00:00<00:00, 413.91it/s, batch loss=21.7]\n",
      "[16th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.37it/s, batch loss=20.4]\n",
      "[17th epoch]: 100%|██████████| 12/12 [00:00<00:00, 421.81it/s, batch loss=19.1]\n",
      "[18th epoch]: 100%|██████████| 12/12 [00:00<00:00, 403.24it/s, batch loss=18]\n",
      "[19th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.10it/s, batch loss=17]\n",
      "[20th epoch]: 100%|██████████| 12/12 [00:00<00:00, 404.07it/s, batch loss=16.1]\n",
      "[21th epoch]: 100%|██████████| 12/12 [00:00<00:00, 403.17it/s, batch loss=15.3]\n",
      "[22th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.11it/s, batch loss=14.5]\n",
      "[23th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.88it/s, batch loss=13.8]\n",
      "[24th epoch]: 100%|██████████| 12/12 [00:00<00:00, 426.85it/s, batch loss=13.2]\n",
      "[25th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.58it/s, batch loss=12.6]\n",
      "[26th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.33it/s, batch loss=12.1]\n",
      "[27th epoch]: 100%|██████████| 12/12 [00:00<00:00, 415.98it/s, batch loss=11.6]\n",
      "[28th epoch]: 100%|██████████| 12/12 [00:00<00:00, 407.25it/s, batch loss=11.2]\n",
      "[29th epoch]: 100%|██████████| 12/12 [00:00<00:00, 409.33it/s, batch loss=10.8]\n",
      "[30th epoch]: 100%|██████████| 12/12 [00:00<00:00, 392.71it/s, batch loss=10.4]\n",
      "[31th epoch]: 100%|██████████| 12/12 [00:00<00:00, 396.18it/s, batch loss=10]\n",
      "[32th epoch]: 100%|██████████| 12/12 [00:00<00:00, 414.27it/s, batch loss=9.7]\n",
      "[33th epoch]: 100%|██████████| 12/12 [00:00<00:00, 396.08it/s, batch loss=9.38]\n",
      "[34th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.00it/s, batch loss=9.09]\n",
      "[35th epoch]: 100%|██████████| 12/12 [00:00<00:00, 407.81it/s, batch loss=8.81]\n",
      "[36th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.87it/s, batch loss=8.55]\n",
      "[37th epoch]: 100%|██████████| 12/12 [00:00<00:00, 343.84it/s, batch loss=8.31]\n",
      "[38th epoch]: 100%|██████████| 12/12 [00:00<00:00, 414.48it/s, batch loss=8.08]\n",
      "[39th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.13it/s, batch loss=7.86]\n",
      "[40th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.95it/s, batch loss=7.65]\n",
      "[41th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.30it/s, batch loss=7.46]\n",
      "[42th epoch]: 100%|██████████| 12/12 [00:00<00:00, 397.18it/s, batch loss=7.27]\n",
      "[43th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.83it/s, batch loss=7.1]\n",
      "[44th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.15it/s, batch loss=6.93]\n",
      "[45th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.02it/s, batch loss=6.77]\n",
      "[46th epoch]: 100%|██████████| 12/12 [00:00<00:00, 380.39it/s, batch loss=6.62]\n",
      "[47th epoch]: 100%|██████████| 12/12 [00:00<00:00, 414.14it/s, batch loss=6.48]\n",
      "[48th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.77it/s, batch loss=6.35]\n",
      "[49th epoch]: 100%|██████████| 12/12 [00:00<00:00, 433.73it/s, batch loss=6.22]\n",
      "[50th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.97it/s, batch loss=6.1]\n",
      "[51th epoch]: 100%|██████████| 12/12 [00:00<00:00, 426.36it/s, batch loss=5.99]\n",
      "[52th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.61it/s, batch loss=5.88]\n",
      "[53th epoch]: 100%|██████████| 12/12 [00:00<00:00, 395.54it/s, batch loss=5.78]\n",
      "[54th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.01it/s, batch loss=5.68]\n",
      "[55th epoch]: 100%|██████████| 12/12 [00:00<00:00, 426.23it/s, batch loss=5.59]\n",
      "[56th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.25it/s, batch loss=5.51]\n",
      "[57th epoch]: 100%|██████████| 12/12 [00:00<00:00, 334.21it/s, batch loss=5.43]\n",
      "[58th epoch]: 100%|██████████| 12/12 [00:00<00:00, 353.67it/s, batch loss=5.35]\n",
      "[59th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.93it/s, batch loss=5.28]\n",
      "[60th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.85it/s, batch loss=5.21]\n",
      "[61th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.95it/s, batch loss=5.15]\n",
      "[62th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.81it/s, batch loss=5.09]\n",
      "[63th epoch]: 100%|██████████| 12/12 [00:00<00:00, 367.16it/s, batch loss=5.03]\n",
      "[64th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.36it/s, batch loss=4.98]\n",
      "[65th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.26it/s, batch loss=4.93]\n",
      "[66th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.98it/s, batch loss=4.89]\n",
      "[67th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.64it/s, batch loss=4.84]\n",
      "[68th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.70it/s, batch loss=4.8]\n",
      "[69th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.45it/s, batch loss=4.76]\n",
      "[70th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.11it/s, batch loss=4.73]\n",
      "[71th epoch]: 100%|██████████| 12/12 [00:00<00:00, 429.81it/s, batch loss=4.7]\n",
      "[72th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.76it/s, batch loss=4.67]\n",
      "[73th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.40it/s, batch loss=4.64]\n",
      "[74th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.41it/s, batch loss=4.61]\n",
      "[75th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.78it/s, batch loss=4.59]\n",
      "[76th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.89it/s, batch loss=4.57]\n",
      "[77th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.31it/s, batch loss=4.55]\n",
      "[78th epoch]: 100%|██████████| 12/12 [00:00<00:00, 433.31it/s, batch loss=4.53]\n",
      "[79th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.51it/s, batch loss=4.51]\n",
      "[80th epoch]: 100%|██████████| 12/12 [00:00<00:00, 432.93it/s, batch loss=4.5]\n",
      "[81th epoch]: 100%|██████████| 12/12 [00:00<00:00, 402.05it/s, batch loss=4.48]\n",
      "[82th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.63it/s, batch loss=4.47]\n",
      "[83th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.29it/s, batch loss=4.46]\n",
      "[84th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.95it/s, batch loss=4.45]\n",
      "[85th epoch]: 100%|██████████| 12/12 [00:00<00:00, 451.38it/s, batch loss=4.44]\n",
      "[86th epoch]: 100%|██████████| 12/12 [00:00<00:00, 448.43it/s, batch loss=4.44]\n",
      "[87th epoch]: 100%|██████████| 12/12 [00:00<00:00, 426.81it/s, batch loss=4.43]\n",
      "[88th epoch]: 100%|██████████| 12/12 [00:00<00:00, 428.37it/s, batch loss=4.43]\n",
      "[89th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.29it/s, batch loss=4.42]\n",
      "[90th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.99it/s, batch loss=4.42]\n",
      "[91th epoch]: 100%|██████████| 12/12 [00:00<00:00, 404.95it/s, batch loss=4.42]\n",
      "[92th epoch]: 100%|██████████| 12/12 [00:00<00:00, 416.38it/s, batch loss=4.42]\n",
      "[93th epoch]: 100%|██████████| 12/12 [00:00<00:00, 395.38it/s, batch loss=4.42]\n",
      "[94th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.78it/s, batch loss=4.42]\n",
      "[95th epoch]: 100%|██████████| 12/12 [00:00<00:00, 422.45it/s, batch loss=4.42]\n",
      "[96th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.04it/s, batch loss=4.43]\n",
      "[97th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.14it/s, batch loss=4.43]\n",
      "[98th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.17it/s, batch loss=4.44]\n",
      "[99th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.02it/s, batch loss=4.44]\n",
      "[100th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.85it/s, batch loss=4.45]\n",
      "[101th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.72it/s, batch loss=4.45]\n",
      "[102th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.30it/s, batch loss=4.46]\n",
      "[103th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.99it/s, batch loss=4.47]\n",
      "[104th epoch]: 100%|██████████| 12/12 [00:00<00:00, 384.96it/s, batch loss=4.48]\n",
      "[105th epoch]: 100%|██████████| 12/12 [00:00<00:00, 426.00it/s, batch loss=4.49]\n",
      "[106th epoch]: 100%|██████████| 12/12 [00:00<00:00, 438.09it/s, batch loss=4.5]\n",
      "[107th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.79it/s, batch loss=4.51]\n",
      "[108th epoch]: 100%|██████████| 12/12 [00:00<00:00, 425.71it/s, batch loss=4.52]\n",
      "[109th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.60it/s, batch loss=4.53]\n",
      "[110th epoch]: 100%|██████████| 12/12 [00:00<00:00, 424.21it/s, batch loss=4.54]\n",
      "[111th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.79it/s, batch loss=4.55]\n",
      "[112th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.51it/s, batch loss=4.56]\n",
      "[113th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.59it/s, batch loss=4.58]\n",
      "[114th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.73it/s, batch loss=4.59]\n",
      "[115th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.68it/s, batch loss=4.6]\n",
      "[116th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.13it/s, batch loss=4.62]\n",
      "[117th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.63it/s, batch loss=4.63]\n",
      "[118th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.41it/s, batch loss=4.65]\n",
      "[119th epoch]: 100%|██████████| 12/12 [00:00<00:00, 437.97it/s, batch loss=4.66]\n",
      "[120th epoch]: 100%|██████████| 12/12 [00:00<00:00, 421.84it/s, batch loss=4.68]\n",
      "[121th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.82it/s, batch loss=4.69]\n",
      "[122th epoch]: 100%|██████████| 12/12 [00:00<00:00, 425.41it/s, batch loss=4.71]\n",
      "[123th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.09it/s, batch loss=4.73]\n",
      "[124th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.92it/s, batch loss=4.74]\n",
      "[125th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.40it/s, batch loss=4.76]\n",
      "[126th epoch]: 100%|██████████| 12/12 [00:00<00:00, 446.10it/s, batch loss=4.78]\n",
      "[127th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.70it/s, batch loss=4.8]\n",
      "[128th epoch]: 100%|██████████| 12/12 [00:00<00:00, 333.41it/s, batch loss=4.81]\n",
      "[129th epoch]: 100%|██████████| 12/12 [00:00<00:00, 344.13it/s, batch loss=4.83]\n",
      "[130th epoch]: 100%|██████████| 12/12 [00:00<00:00, 338.84it/s, batch loss=4.85]\n",
      "[131th epoch]: 100%|██████████| 12/12 [00:00<00:00, 356.71it/s, batch loss=4.87]\n",
      "[132th epoch]: 100%|██████████| 12/12 [00:00<00:00, 338.23it/s, batch loss=4.89]\n",
      "[133th epoch]: 100%|██████████| 12/12 [00:00<00:00, 383.35it/s, batch loss=4.91]\n",
      "[134th epoch]: 100%|██████████| 12/12 [00:00<00:00, 312.62it/s, batch loss=4.93]\n",
      "[135th epoch]: 100%|██████████| 12/12 [00:00<00:00, 343.46it/s, batch loss=4.95]\n",
      "[136th epoch]: 100%|██████████| 12/12 [00:00<00:00, 321.59it/s, batch loss=4.97]\n",
      "[137th epoch]: 100%|██████████| 12/12 [00:00<00:00, 343.53it/s, batch loss=4.99]\n",
      "[138th epoch]: 100%|██████████| 12/12 [00:00<00:00, 335.89it/s, batch loss=5.01]\n",
      "[139th epoch]: 100%|██████████| 12/12 [00:00<00:00, 350.22it/s, batch loss=5.03]\n",
      "[140th epoch]: 100%|██████████| 12/12 [00:00<00:00, 360.69it/s, batch loss=5.05]\n",
      "[141th epoch]: 100%|██████████| 12/12 [00:00<00:00, 353.35it/s, batch loss=5.07]\n",
      "[142th epoch]: 100%|██████████| 12/12 [00:00<00:00, 335.95it/s, batch loss=5.09]\n",
      "[143th epoch]: 100%|██████████| 12/12 [00:00<00:00, 319.46it/s, batch loss=5.12]\n",
      "[144th epoch]: 100%|██████████| 12/12 [00:00<00:00, 321.29it/s, batch loss=5.14]\n",
      "[145th epoch]: 100%|██████████| 12/12 [00:00<00:00, 376.48it/s, batch loss=5.16]\n",
      "[146th epoch]: 100%|██████████| 12/12 [00:00<00:00, 372.01it/s, batch loss=5.18]\n",
      "[147th epoch]: 100%|██████████| 12/12 [00:00<00:00, 454.07it/s, batch loss=5.21]\n",
      "[148th epoch]: 100%|██████████| 12/12 [00:00<00:00, 410.45it/s, batch loss=5.23]\n",
      "[149th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.76it/s, batch loss=5.25]\n",
      "[150th epoch]: 100%|██████████| 12/12 [00:00<00:00, 408.11it/s, batch loss=5.27]\n",
      "[151th epoch]: 100%|██████████| 12/12 [00:00<00:00, 450.22it/s, batch loss=5.3]\n",
      "[152th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.90it/s, batch loss=5.32]\n",
      "[153th epoch]: 100%|██████████| 12/12 [00:00<00:00, 450.60it/s, batch loss=5.34]\n",
      "[154th epoch]: 100%|██████████| 12/12 [00:00<00:00, 419.03it/s, batch loss=5.37]\n",
      "[155th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.16it/s, batch loss=5.39]\n",
      "[156th epoch]: 100%|██████████| 12/12 [00:00<00:00, 457.42it/s, batch loss=5.42]\n",
      "[157th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.62it/s, batch loss=5.44]\n",
      "[158th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.97it/s, batch loss=5.47]\n",
      "[159th epoch]: 100%|██████████| 12/12 [00:00<00:00, 401.68it/s, batch loss=5.49]\n",
      "[160th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.95it/s, batch loss=5.52]\n",
      "[161th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.89it/s, batch loss=5.54]\n",
      "[162th epoch]: 100%|██████████| 12/12 [00:00<00:00, 439.67it/s, batch loss=5.57]\n",
      "[163th epoch]: 100%|██████████| 12/12 [00:00<00:00, 397.77it/s, batch loss=5.59]\n",
      "[164th epoch]: 100%|██████████| 12/12 [00:00<00:00, 432.78it/s, batch loss=5.62]\n",
      "[165th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.19it/s, batch loss=5.64]\n",
      "[166th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.97it/s, batch loss=5.67]\n",
      "[167th epoch]: 100%|██████████| 12/12 [00:00<00:00, 420.36it/s, batch loss=5.7]\n",
      "[168th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.16it/s, batch loss=5.72]\n",
      "[169th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.66it/s, batch loss=5.75]\n",
      "[170th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.24it/s, batch loss=5.77]\n",
      "[171th epoch]: 100%|██████████| 12/12 [00:00<00:00, 436.44it/s, batch loss=5.8]\n",
      "[172th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.56it/s, batch loss=5.83]\n",
      "[173th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.77it/s, batch loss=5.86]\n",
      "[174th epoch]: 100%|██████████| 12/12 [00:00<00:00, 423.61it/s, batch loss=5.88]\n",
      "[175th epoch]: 100%|██████████| 12/12 [00:00<00:00, 421.84it/s, batch loss=5.91]\n",
      "[176th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.67it/s, batch loss=5.94]\n",
      "[177th epoch]: 100%|██████████| 12/12 [00:00<00:00, 354.48it/s, batch loss=5.97]\n",
      "[178th epoch]: 100%|██████████| 12/12 [00:00<00:00, 443.07it/s, batch loss=5.99]\n",
      "[179th epoch]: 100%|██████████| 12/12 [00:00<00:00, 430.28it/s, batch loss=6.02]\n",
      "[180th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.01it/s, batch loss=6.05]\n",
      "[181th epoch]: 100%|██████████| 12/12 [00:00<00:00, 406.18it/s, batch loss=6.08]\n",
      "[182th epoch]: 100%|██████████| 12/12 [00:00<00:00, 449.37it/s, batch loss=6.11]\n",
      "[183th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.54it/s, batch loss=6.14]\n",
      "[184th epoch]: 100%|██████████| 12/12 [00:00<00:00, 412.91it/s, batch loss=6.17]\n",
      "[185th epoch]: 100%|██████████| 12/12 [00:00<00:00, 442.30it/s, batch loss=6.2]\n",
      "[186th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.13it/s, batch loss=6.23]\n",
      "[187th epoch]: 100%|██████████| 12/12 [00:00<00:00, 445.19it/s, batch loss=6.25]\n",
      "[188th epoch]: 100%|██████████| 12/12 [00:00<00:00, 427.46it/s, batch loss=6.28]\n",
      "[189th epoch]: 100%|██████████| 12/12 [00:00<00:00, 434.22it/s, batch loss=6.31]\n",
      "[190th epoch]: 100%|██████████| 12/12 [00:00<00:00, 440.37it/s, batch loss=6.34]\n",
      "[191th epoch]: 100%|██████████| 12/12 [00:00<00:00, 431.89it/s, batch loss=6.37]\n",
      "[192th epoch]: 100%|██████████| 12/12 [00:00<00:00, 415.98it/s, batch loss=6.4]\n",
      "[193th epoch]: 100%|██████████| 12/12 [00:00<00:00, 375.14it/s, batch loss=6.44]\n",
      "[194th epoch]: 100%|██████████| 12/12 [00:00<00:00, 353.35it/s, batch loss=6.47]\n",
      "[195th epoch]: 100%|██████████| 12/12 [00:00<00:00, 411.95it/s, batch loss=6.5]\n",
      "[196th epoch]: 100%|██████████| 12/12 [00:00<00:00, 402.94it/s, batch loss=6.53]\n",
      "[197th epoch]: 100%|██████████| 12/12 [00:00<00:00, 441.50it/s, batch loss=6.56]\n",
      "[198th epoch]: 100%|██████████| 12/12 [00:00<00:00, 444.13it/s, batch loss=6.59]\n",
      "[199th epoch]: 100%|██████████| 12/12 [00:00<00:00, 435.04it/s, batch loss=6.62]\n",
      "                                                 \r"
     ]
    }
   ],
   "source": [
    "student = Student(num_features, num_classes)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4)\n",
    "\n",
    "policy = AdvancedDistillationPolicy(n_epochs)\n",
    "\n",
    "distiller = CustomDistiller(teacher, student, policy, optimizer)\n",
    "distiller(train_loader, val_loader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ce520ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        57\n",
      "           1       1.00      1.00      1.00        55\n",
      "           2       0.98      1.00      0.99        45\n",
      "           3       1.00      0.98      0.99        47\n",
      "           4       1.00      0.98      0.99        46\n",
      "\n",
      "    accuracy                           0.99       250\n",
      "   macro avg       0.99      0.99      0.99       250\n",
      "weighted avg       0.99      0.99      0.99       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(student, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20218dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
